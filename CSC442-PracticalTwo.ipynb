{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tutorial Section on SKLEARN\n",
    "\n",
    "\n",
    "Scikit-learn, also known as sklearn, is an open-source, robust Python machine learning library. It was created to help simplify the process of implementing machine learning and statistical models in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install Sklearn\n",
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would used the wine datasets for this tutorial, the dataset's task involves classifying wines into one of  three cultivars. The three cultivars (classes) represented in the sklearn wine dataset correspond to different types or varieties of wine grapes. These cultivars are often associated with specific wine-producing regions and have distinct characteristics that influence the flavors, aromas, and overall profiles of the wines produced from them. (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load required packages\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the wine dataset from sklearn\n",
    "data = load_wine()\n",
    "wine_df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "wine_df['target'] = data.target  # Add target column for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Description\n",
    "print(wine_df.info())  # Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wine_df.shape[0])  # Number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wine_df.shape[1])  # Number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wine_df.columns)  # Names of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target (class) distribution\n",
    "print(wine_df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics of 'alcohol' content\n",
    "print(wine_df['alcohol'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick bar plot of alcohol content\n",
    "plt.figure(figsize=(20, 6))\n",
    "sns.countplot(x='alcohol', data=wine_df)\n",
    "plt.title('Distribution of Alcohol Content')\n",
    "plt.show()\n",
    "\n",
    "'Plots every single value from the minimum to the maximum'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of 'alcohol' content\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(wine_df['alcohol'], bins=30, kde=True) \n",
    "plt.title('Histogram of Alcohol Content')\n",
    "plt.xlabel('Alcohol Content (%)')\n",
    "plt.show()\n",
    "'''\n",
    "By setting the bin, you limit the number of bars, think of it this way\n",
    "you have 178 balls but you put them into 30 baskets, the balls closet \n",
    "to each other are put in the same basket\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of 'alcohol' content\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x=wine_df['alcohol'])\n",
    "plt.title('Boxplot of Alcohol Content')\n",
    "plt.xlabel('Alcohol Content (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot of 'target' (class)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='target', data=wine_df)\n",
    "plt.title('Distribution of Wine Classes')\n",
    "plt.xlabel('Wine Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a two-way table\n",
    "pd.crosstab(wine_df['alcohol'], wine_df['target'])\n",
    "\n",
    "#This shows how many members of each class have a specific alcohol content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of 'alcohol' content vs 'flavanoids' with target classes\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='alcohol', y='flavanoids', hue='target', data=wine_df)\n",
    "plt.title('Alcohol Content vs flavanoids with Target Classes')\n",
    "plt.xlabel('Alcohol Content (%)')\n",
    "plt.ylabel('flavanoids')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-test to compare 'alcohol' content between different target classes\n",
    "class_0_alcohol = wine_df.loc[wine_df['target'] == 0, 'alcohol']\n",
    "class_1_alcohol = wine_df.loc[wine_df['target'] == 1, 'alcohol']\n",
    "from scipy.stats import ttest_ind\n",
    "t_stat, p_value = ttest_ind(class_0_alcohol, class_1_alcohol, alternative='two-sided', equal_var=False)\n",
    "print(\"T-test (two-sided) p-value:\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a preview\n",
    "wine_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Exploration\n",
    "wine_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing\n",
    "\n",
    "Data processing is a vital step in the machine learning workflow because data from the real world is messy. It may contain: \n",
    "\n",
    "Missing values,\n",
    "Redundant values\n",
    "Outliers\n",
    "Errors\n",
    "Noise\n",
    "\n",
    "You must deal with all of this before feeding the data to a machine learning model; otherwise, the model will incorporate these mistakes into its approximation function – it will learn to make mistakes on new instances. This is what formed the famous machine learning saying, “Garbage in, garbage out.” \n",
    "\n",
    "Another reason is that machine learning models typically require numeric data.  \n",
    "\n",
    "Other than our data being on different scales, there’s not much else wrong with our data at first glance. To combat this problem, let’s standardize the features using sklearn’s StandardScaler class; this will ensure the mean of each feature is approximately equal to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split data into features and label \n",
    "features = wine_df[data.feature_names].copy()\n",
    "labels = wine_df[\"target\"].copy() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate scaler and fit on features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features)\n",
    "\n",
    "# Transform features\n",
    "X_scaled = scaler.transform(features.values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first instance of scaled data\n",
    "print(X_scaled[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first instance of unscaled data\n",
    "print(features.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Trainning --- Spliting the dataset\n",
    "\n",
    "Before a machine learning model can make predictions, it must be trained on a set of data to learn an approximation function. \n",
    "\n",
    "There are several ways to split data into train and test sets, but scikit-learn has a built-in function to do this on our behalf called train_test_split(). \n",
    "\n",
    "We’ll use this function to split our data such that 70% is used to train the model and 30% is used to evaluate the model's ability to generalize to unseen instances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into train and test\n",
    "X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(X_scaled,\n",
    "                                                                  labels,\n",
    "                                                             train_size=.7,\n",
    "                                                           random_state=0)\n",
    "\n",
    "# Check the splits are correct\n",
    "print(f\"Train size: {round(len(X_train_scaled) / len(features) * 100)}% \\n\\\n",
    "Test size: {round(len(X_test_scaled) / len(features) * 100)}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the model\n",
    "Thanks to sklearn, building a machine learning model is extremely simple. \n",
    "\n",
    "We are going to build three models to predict the class of wine: \n",
    "\n",
    "Logistic regression (https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)\n",
    "Support vector machine (https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)\n",
    "Decision tree classifier(https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Initializing the model \n",
    "logistic_regression = LogisticRegression()\n",
    "# Training the models\n",
    "logistic_regression.fit(X_train_scaled, y_train)\n",
    "# Making predictions with the model\n",
    "log_reg_preds = logistic_regression.predict(X_test_scaled)\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, log_reg_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "# Initializing the model \n",
    "svm = SVC()\n",
    "# Training the models\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "# Making predictions with the model\n",
    "svm_preds = svm.predict(X_test_scaled)\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, svm_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Initializing the model \n",
    "tree = DecisionTreeClassifier(random_state=0) #Why do use think we are using random state and why arent we using it for the others, Some Machine learning \n",
    "                                              #algorithms are prone to randomization and would not produce the same result if random state is not decleared\n",
    "# Training the models\n",
    "tree.fit(X_train_scaled, y_train)\n",
    "# Making predictions with the model\n",
    "tree_preds = tree.predict(X_test_scaled)\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, tree_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks\n",
    "\n",
    "1. Change the random state of the Decision tree classifier (for example set it to 42), what was the effect of this change\n",
    "2. Conduct an experiment using the 3 Machine learning algorithms onthe Sklearn breast cancer dataset (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer)\n",
    "3. what is the performance of the random forest algorithm on the breast cancer dataset (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is a type of unsupervised learning technique used to group data points or objects based on their similarity. The goal of clustering is to identify inherent patterns or structures in the data without prior knowledge of true labels. Clustering algorithms partition the data into groups or clusters such that data points within the same cluster are more similar to each other than to those in other clusters.\n",
    "\n",
    "In this tutorial we would use two clustering algorithms: K-means and Agglomerative Clustering \n",
    "\n",
    "We would use metrics such as Silhouette Score, Davies-Bouldin Index, Calinski-Harabasz Score and Adjusted Rand Index (ARI)\n",
    "\n",
    "K-means Clustering:\n",
    " K-means is a popular centroid-based clustering algorithm. It partitions the data into K clusters by iteratively assigning data points to the nearest cluster centroid and updating the centroids based on the mean of data points assigned to each cluster.\n",
    "\n",
    "\n",
    "Agglomerative Clustering:\n",
    "Agglomerative clustering is a hierarchical clustering method that starts with each data point as a separate cluster and merges clusters iteratively based on a linkage criterion (e.g., distance between clusters)\n",
    "\n",
    "\n",
    "Performance Metrics:\n",
    "Silhouette Score: Measures how similar a data point is to its own cluster compared to other clusters. Higher score indicates dense and well-separated clusters.\n",
    "\n",
    "Davies-Bouldin Index: Computes the average similarity between each cluster and its most similar cluster, where lower values indicate better clustering.\n",
    "\n",
    "Calinski-Harabasz Score: Ratio of within-cluster dispersion to between-cluster dispersion, higher values indicate better-defined clusters.\n",
    "\n",
    "Adjusted Rand Index (ARI): Compares the similarity of true cluster assignments with the clustering results, providing a measure of cluster accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, adjusted_rand_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the wine dataset\n",
    "data = load_wine()\n",
    "X = data.data  # Features\n",
    "y = data.target  # True labels (for adjusted Rand index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the clustering algorithms\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the clustering algorithms to the scaled data\n",
    "kmeans_labels = kmeans.fit_predict(X_scaled)\n",
    "agg_labels = agg_clustering.fit_predict(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate K-means clustering performance using multiple metrics\n",
    "metrics_kmeans = {\n",
    "    'Silhouette Score': silhouette_score(X_scaled, kmeans_labels),\n",
    "    'Davies-Bouldin Index': davies_bouldin_score(X_scaled, kmeans_labels),\n",
    "    'Calinski-Harabasz Score': calinski_harabasz_score(X_scaled, kmeans_labels),\n",
    "    'Adjusted Rand Index': adjusted_rand_score(y, kmeans_labels)  # Using true labels for ARI\n",
    "}\n",
    "\n",
    "print(\"K-means Clustering Performance:\")\n",
    "for metric_name, metric_value in metrics_kmeans.items():\n",
    "    print(f\"{metric_name}: {metric_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Agglomerative Clustering performance using multiple metrics\n",
    "metrics_agg = {\n",
    "    'Silhouette Score': silhouette_score(X_scaled, agg_labels),\n",
    "    'Davies-Bouldin Index': davies_bouldin_score(X_scaled, agg_labels),\n",
    "    'Calinski-Harabasz Score': calinski_harabasz_score(X_scaled, agg_labels),\n",
    "    'Adjusted Rand Index': adjusted_rand_score(y, agg_labels)  # Using true labels for ARI\n",
    "}\n",
    "\n",
    "print(\"\\nAgglomerative Clustering Performance:\")\n",
    "for metric_name, metric_value in metrics_agg.items():\n",
    "    print(f\"{metric_name}: {metric_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Knew that there were 3 cultivars in the wine dataset, what if we didnt there are several methods that can be used to determine the optimal number of clusters if the number of clusters are not known. Some of them include Elbow Method, Silhouette Analysis, Gap Statistic method, and Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elbow Method:\n",
    "The elbow method involves plotting the within-cluster sum of squares (inertia) against the number of clusters (K) and identifying the \"elbow\" point where the rate of decrease in inertia sharply decreases. This point represents a good estimate for the optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertias = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1, 11), inertias, marker='o')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette Analysis:\n",
    "Silhouette analysis measures how well each data point fits into its assigned cluster and can be used to determine the optimal number of clusters. The highest average silhouette score across different numbers of clusters indicates the optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_scores = []\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    silhouette_avg = silhouette_score(X_scaled, labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "plt.plot(range(2, 11), silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Analysis for Optimal K')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gap Statistic:\n",
    "The gap statistic compares the within-cluster dispersion of the data to a reference null distribution and helps identify the optimal number of clusters by maximizing the gap statistic value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "def compute_gap_statistic(data, k_range, n_ref_samples=10, random_seed=None):\n",
    "    \"\"\"\n",
    "    Compute the Gap Statistic for estimating the optimal number of clusters.\n",
    "    \n",
    "    Parameters:\n",
    "        data (numpy.ndarray): Input data matrix (n_samples, n_features).\n",
    "        k_range (list): List of integers specifying the range of k values (number of clusters) to evaluate.\n",
    "        n_ref_samples (int): Number of reference samples to generate for calculating the reference distribution.\n",
    "        random_seed (int): Random seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Tuple containing the calculated gap statistics and standard deviations for each k value.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Initialize arrays to store gap statistics and standard deviations\n",
    "    gap_stats = []\n",
    "    gap_stds = []\n",
    "    \n",
    "    for k in k_range:\n",
    "        # Fit KMeans clustering to the data\n",
    "        kmeans_model = KMeans(n_clusters=k, random_state=random_seed)\n",
    "        kmeans_model.fit(data)\n",
    "        \n",
    "        # Calculate the within-cluster dispersion (log of sum of square distances)\n",
    "        Wk = np.log(kmeans_model.inertia_)\n",
    "        \n",
    "        # Generate reference datasets and calculate their within-cluster dispersions\n",
    "        ref_Wks = []\n",
    "        for _ in range(n_ref_samples):\n",
    "            # Generate reference dataset with the same shape and distribution as the original data\n",
    "            ref_data = np.random.rand(*data.shape)\n",
    "            \n",
    "            # Fit KMeans to reference dataset\n",
    "            ref_kmeans_model = KMeans(n_clusters=k, random_state=random_seed)\n",
    "            ref_kmeans_model.fit(ref_data)\n",
    "            \n",
    "            # Calculate within-cluster dispersion of reference dataset\n",
    "            ref_Wk = np.log(ref_kmeans_model.inertia_)\n",
    "            ref_Wks.append(ref_Wk)\n",
    "        \n",
    "        # Calculate Gap Statistic and its standard deviation\n",
    "        gap_stat = np.mean(ref_Wks) - Wk\n",
    "        gap_std = np.std(ref_Wks) * np.sqrt(1 + 1/n_ref_samples)\n",
    "        \n",
    "        gap_stats.append(gap_stat)\n",
    "        gap_stds.append(gap_std)\n",
    "    \n",
    "    return np.array(gap_stats), np.array(gap_stds)\n",
    "\n",
    "\n",
    "# Define the range of k values (number of clusters) to evaluate\n",
    "k_range = range(1, 11)\n",
    "\n",
    "# Compute Gap Statistic for the range of k values\n",
    "gap_stats, gap_stds = compute_gap_statistic(X_scaled, k_range, n_ref_samples=10, random_seed=42)\n",
    "\n",
    "# Plotting the Gap Statistic curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, gap_stats, marker='o', color='b', label='Gap Statistic')\n",
    "plt.errorbar(k_range, gap_stats, yerr=gap_stds, fmt='-o', color='b', alpha=0.5, label='Gap Statistic with Std Dev')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Gap Statistic')\n",
    "plt.title('Gap Statistic for Optimal k')\n",
    "plt.xticks(k_range)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical Clustering (Dendrogram):\n",
    "Hierarchical clustering can provide insights into the underlying structure of the data by visualizing a dendrogram, which represents the hierarchical merging of clusters. The height at which branches are merged can help determine the number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "Z = linkage(X_scaled, method='ward')\n",
    "plt.figure(figsize=(12, 8))\n",
    "dendrogram(Z)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: repeat the clustering tasks for the Sklearn BRCA dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
